{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e2facb-9371-4916-a226-9e982f014767",
   "metadata": {},
   "source": [
    "# Fine-tune BERT for tissue and cell type NER\n",
    "Fine tune existing bioNER models for tissue and cell type identification in abstracts. Find optimal parameters and save the final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfe7d8d6-97d7-4cf6-95ce-9a0d584b3144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Sequence, Value, ClassLabel\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import random\n",
    "from iob_functions import *\n",
    "\n",
    "random.seed(6002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93062f26-3f33-4a7a-9095-9db3bffef433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIR = \"../data/\"\n",
    "\n",
    "training_f = process_tab_delim_iob(BASE_DIR + 'tags/fulltext_iob/fulltext_tissues_train.iob')\n",
    "training_a = process_tab_delim_iob(BASE_DIR + 'tags/abstract_iob/abstract_tissues_train.iob')\n",
    "training = {'sentences': training_f['sentences'] + training_a['sentences'], 'tags': training_f['tags'] + training_a['tags']}\n",
    "\n",
    "valid_f = process_tab_delim_iob(BASE_DIR + 'tags/fulltext_iob/fulltext_tissues_validation.iob')\n",
    "valid_a = process_tab_delim_iob(BASE_DIR + 'tags/abstract_iob/abstract_tissues_validation.iob')\n",
    "validation = {'sentences': valid_f['sentences'] + valid_a['sentences'], 'tags': valid_f['tags'] + valid_a['tags']}\n",
    "\n",
    "test_f = process_tab_delim_iob(BASE_DIR + 'tags/fulltext_iob/fulltext_tissues_test.iob')\n",
    "test_a = process_tab_delim_iob(BASE_DIR + 'tags/abstract_iob/abstract_tissues_test.iob')\n",
    "test = {'sentences': test_f['sentences'] + test_a['sentences'], 'tags': test_f['tags'] + test_a['tags']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a741277-eeb9-441f-bedb-622c695beee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = Features({\"tokens\": Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "                     \"tags\": Sequence(feature=ClassLabel(names=[\"O\", \"B-CELL_TYPE\", \"I-CELL_TYPE\", \"B-TISSUE\", \"I-TISSUE\"]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e457c5f-de08-4c67-8430-bdb47d1b74b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 80750,\n",
       " 'B-CELL_TYPE': 339,\n",
       " 'I-CELL_TYPE': 213,\n",
       " 'B-TISSUE': 415,\n",
       " 'I-TISSUE': 88}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_stats(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33eec4dc-d374-40ed-8566-d1f9c1c7398a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 484660,\n",
       " 'B-CELL_TYPE': 3611,\n",
       " 'I-CELL_TYPE': 2654,\n",
       " 'B-TISSUE': 3369,\n",
       " 'I-TISSUE': 559}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_stats(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "344f7fbf-a6dd-4a5d-9089-b33d3fdfa49d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 134071,\n",
       " 'B-CELL_TYPE': 1383,\n",
       " 'B-TISSUE': 985,\n",
       " 'I-CELL_TYPE': 917,\n",
       " 'I-TISSUE': 160}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_stats(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78dd5cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds = Dataset.from_dict({\"tokens\": training['sentences'], \"tags\": training['tags']}, features=features)\n",
    "validation_ds = Dataset.from_dict({\"tokens\": validation['sentences'], \"tags\": validation['tags']}, features=features)\n",
    "test_ds = Dataset.from_dict({\"tokens\": test['sentences'], \"tags\": test['tags']}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76db193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = training_ds.features[\"tags\"].feature\n",
    "tag_list = training_ds.features[\"tags\"].feature.names\n",
    "id2tag = {idx: tag for idx, tag in enumerate(all_tags.names)}\n",
    "tag2id = {tag: idx for idx, tag in enumerate(all_tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28fa7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the best performing model\n",
    "m = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract'\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acba6d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/docs/transformers/tasks/token_classification\n",
    "def tokenize_and_align_labels(data, tknzr, max_length=50):\n",
    "    tokenized_inputs = tknzr(data['tokens'], truncation=True, is_split_into_words=True, max_length=max_length)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(data['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "360b7e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [tag_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [tag_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    f1res = precision_recall_fscore_support(flatten(true_labels), flatten(true_predictions), labels=all_tags.names)\n",
    "\n",
    "    df = list(zip(all_tags.names, f1res[2], f1res[0], f1res[1]))\n",
    "    df = pd.DataFrame(df, columns = ['Level', 'F1-Score', 'Precision', 'Recall'])   \n",
    "    print(df)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c64cc6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea648d0e5ef441297dc59b2c4556a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffa54fb952847a484e7b845ee4e2cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3499 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59143bf4bdc74ba98c59289a8dcb1a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12875' max='12875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12875/12875 16:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Cell Type</th>\n",
       "      <th>Tissue</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>{'precision': 0.7203389830508474, 'recall': 0.7434402332361516, 'f1': 0.7317073170731706, 'number': 343}</td>\n",
       "      <td>{'precision': 0.7926565874730022, 'recall': 0.8822115384615384, 'f1': 0.8350398179749716, 'number': 416}</td>\n",
       "      <td>0.761322</td>\n",
       "      <td>0.819499</td>\n",
       "      <td>0.789340</td>\n",
       "      <td>0.996135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.010256</td>\n",
       "      <td>{'precision': 0.736, 'recall': 0.8046647230320699, 'f1': 0.7688022284122563, 'number': 343}</td>\n",
       "      <td>{'precision': 0.8108108108108109, 'recall': 0.8653846153846154, 'f1': 0.8372093023255813, 'number': 416}</td>\n",
       "      <td>0.776557</td>\n",
       "      <td>0.837945</td>\n",
       "      <td>0.806084</td>\n",
       "      <td>0.996563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.011933</td>\n",
       "      <td>{'precision': 0.7225433526011561, 'recall': 0.7288629737609329, 'f1': 0.725689404934688, 'number': 343}</td>\n",
       "      <td>{'precision': 0.8310185185185185, 'recall': 0.8629807692307693, 'f1': 0.8466981132075472, 'number': 416}</td>\n",
       "      <td>0.782776</td>\n",
       "      <td>0.802372</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.996441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.013901</td>\n",
       "      <td>{'precision': 0.7207446808510638, 'recall': 0.7900874635568513, 'f1': 0.7538247566063978, 'number': 343}</td>\n",
       "      <td>{'precision': 0.8356164383561644, 'recall': 0.8798076923076923, 'f1': 0.8571428571428572, 'number': 416}</td>\n",
       "      <td>0.782555</td>\n",
       "      <td>0.839262</td>\n",
       "      <td>0.809917</td>\n",
       "      <td>0.996625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.016297</td>\n",
       "      <td>{'precision': 0.7384196185286104, 'recall': 0.7900874635568513, 'f1': 0.7633802816901408, 'number': 343}</td>\n",
       "      <td>{'precision': 0.8048245614035088, 'recall': 0.8822115384615384, 'f1': 0.8417431192660549, 'number': 416}</td>\n",
       "      <td>0.775213</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.806574</td>\n",
       "      <td>0.996539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Level  F1-Score  Precision    Recall\n",
      "0            O  0.998407   0.998674  0.998142\n",
      "1  B-CELL_TYPE  0.765832   0.764706  0.766962\n",
      "2  I-CELL_TYPE  0.815348   0.833333  0.798122\n",
      "3     B-TISSUE  0.873563   0.835165  0.915663\n",
      "4     I-TISSUE  0.844920   0.797980  0.897727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Level  F1-Score  Precision    Recall\n",
      "0            O  0.998637   0.998909  0.998365\n",
      "1  B-CELL_TYPE  0.795455   0.767123  0.825959\n",
      "2  I-CELL_TYPE  0.839161   0.833333  0.845070\n",
      "3     B-TISSUE  0.879813   0.852941  0.908434\n",
      "4     I-TISSUE  0.841463   0.907895  0.784091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Level  F1-Score  Precision    Recall\n",
      "0            O  0.998631   0.998514  0.998749\n",
      "1  B-CELL_TYPE  0.775811   0.775811  0.775811\n",
      "2  I-CELL_TYPE  0.802083   0.900585  0.723005\n",
      "3     B-TISSUE  0.880473   0.865116  0.896386\n",
      "4     I-TISSUE  0.826087   0.791667  0.863636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Level  F1-Score  Precision    Recall\n",
      "0            O  0.998680   0.998983  0.998377\n",
      "1  B-CELL_TYPE  0.796610   0.764228  0.831858\n",
      "2  I-CELL_TYPE  0.831354   0.841346  0.821596\n",
      "3     B-TISSUE  0.883117   0.865741  0.901205\n",
      "4     I-TISSUE  0.863388   0.831579  0.897727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Level  F1-Score  Precision    Recall\n",
      "0            O  0.998661   0.998996  0.998327\n",
      "1  B-CELL_TYPE  0.791966   0.770950  0.814159\n",
      "2  I-CELL_TYPE  0.839329   0.857843  0.821596\n",
      "3     B-TISSUE  0.870968   0.834437  0.910843\n",
      "4     I-TISSUE  0.857143   0.829787  0.886364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Level  F1-Score  Precision    Recall\n",
      "0            O  0.997209   0.998533  0.995888\n",
      "1  B-CELL_TYPE  0.833112   0.770603  0.906657\n",
      "2  I-CELL_TYPE  0.881210   0.871795  0.890830\n",
      "3     B-TISSUE  0.886905   0.867119  0.907614\n",
      "4     I-TISSUE  0.772603   0.687805  0.881250\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(m)\n",
    "# dynamically pad sentences to longest length in batch for efficiency\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "train_tokenized = training_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "val_tokenized = validation_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "test_tokenized = test_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    m, num_labels=5, id2label=id2tag, label2id=tag2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model/\" + m,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "test_preds = trainer.predict(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ab3b861-0043-4cdd-9b9c-a45ca2184aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.017371343448758125,\n",
       " 'test_CELL_TYPE': {'precision': 0.7363034316676701,\n",
       "  'recall': 0.8811239193083573,\n",
       "  'f1': 0.8022302394227615,\n",
       "  'number': 1388},\n",
       " 'test_TISSUE': {'precision': 0.8164435946462715,\n",
       "  'recall': 0.867005076142132,\n",
       "  'f1': 0.8409650418513048,\n",
       "  'number': 985},\n",
       " 'test_overall_precision': 0.7672700406353897,\n",
       " 'test_overall_recall': 0.8752633796881585,\n",
       " 'test_overall_f1': 0.8177165354330709,\n",
       " 'test_overall_accuracy': 0.9935248239334148,\n",
       " 'test_runtime': 22.8285,\n",
       " 'test_samples_per_second': 252.009,\n",
       " 'test_steps_per_second': 31.539}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a1b84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final model\n",
    "import os\n",
    "if not os.path.exists(\"../models\"):\n",
    "    os.makedirs(\"../models\")\n",
    "\n",
    "trainer.save_model(\"../models/BiomedNLP-PubMedBERT-base-uncased-abstract-fine-tuned-for-tissue-celltype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9fac1c-fea1-42c8-8b82-3d01aa35a236",
   "metadata": {},
   "source": [
    "### test cases\n",
    "try some test cases to see how the model performs in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe57bef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'TISSUE',\n",
       "  'score': 0.98907346,\n",
       "  'word': 'heart',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': 'TISSUE',\n",
       "  'score': 0.9912765,\n",
       "  'word': 'brain',\n",
       "  'start': 7,\n",
       "  'end': 12},\n",
       " {'entity_group': 'TISSUE',\n",
       "  'score': 0.9949338,\n",
       "  'word': 'lung',\n",
       "  'start': 18,\n",
       "  'end': 22}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tissue_cell_classifier = pipeline(\n",
    "    \"token-classification\", model=\"../models/BiomedNLP-PubMedBERT-base-uncased-abstract-fine-tuned-for-tissue-celltype\", aggregation_strategy=\"simple\"\n",
    ")\n",
    "tissue_cell_classifier(\"Heart, brain, and lung are all samples that we acquired for this analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f62c21e-8b06-4b95-84c0-88cba01c1d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'CELL_TYPE',\n",
       "  'score': 0.88980865,\n",
       "  'word': 'renal cell',\n",
       "  'start': 0,\n",
       "  'end': 10},\n",
       " {'entity_group': 'TISSUE',\n",
       "  'score': 0.99476445,\n",
       "  'word': 'breast',\n",
       "  'start': 25,\n",
       "  'end': 31}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tissue_cell_classifier(\"Renal cell carcinoma and breast cancer have relatively good prognosis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f2c440a-9b8d-441b-b468-8e33d462fbac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tissue_cell_classifier(\"Realistic scRNA-seq Generation with Automatic Cell-Type identification using Introspective Variational Autoencoders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "209b24b1-e217-419c-ab15-449497d35eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'TISSUE',\n",
       "  'score': 0.9656411,\n",
       "  'word': 'blood',\n",
       "  'start': 8,\n",
       "  'end': 13},\n",
       " {'entity_group': 'TISSUE',\n",
       "  'score': 0.77455556,\n",
       "  'word': 'arm',\n",
       "  'start': 28,\n",
       "  'end': 31}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tissue_cell_classifier(\"We took blood from the left arm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b124c25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tissue_cell_classifier(\"This arm of the study included 1000 participants.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
