{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tool NER with different gold standards\n",
    "Fine tune existing bioNER models for biomedical tool identification with two different gold standards and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Sequence, Value, ClassLabel\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from iob_functions import *\n",
    "\n",
    "random.seed(602)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "BASE_DIR = \"../data/\"\n",
    "\n",
    "training = process_tab_delim_iob(BASE_DIR + 'tags/fulltext_iob/fulltext_tools_train.iob')\n",
    "validation = process_tab_delim_iob(BASE_DIR + 'tags/fulltext_iob/fulltext_tools_validation.iob')\n",
    "test = process_tab_delim_iob(BASE_DIR + 'tags/fulltext_iob/fulltext_tools_test.iob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "softcite_train = process_tab_delim_iob(BASE_DIR + 'tags/softcite.iob')\n",
    "softcite_train = combine_tags(softcite_train, 'software', 'TOOL')\n",
    "softcite_train = remove_tag(softcite_train, 'version')\n",
    "\n",
    "softcite_train, softc_validation = split_training(softcite_train, .1) # take 10% of total for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_stats(softcite_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 285730,\n",
       " 'B-TOOL': 2165,\n",
       " 'B-UNS_METHOD': 372,\n",
       " 'I-UNS_METHOD': 363,\n",
       " 'I-TOOL': 764}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_stats(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\"tokens\": Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
    "                     \"tags\": Sequence(feature=ClassLabel(names=[\"O\", \"B-UNS_METHOD\", \"I-UNS_METHOD\", \"B-TOOL\", \"I-TOOL\"]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ds = Dataset.from_dict({\"tokens\": training['sentences'], \"tags\": training['tags']}, features=features)\n",
    "validation_ds = Dataset.from_dict({\"tokens\": validation['sentences'], \"tags\": validation['tags']}, features=features)\n",
    "test_ds = Dataset.from_dict({\"tokens\": test['sentences'], \"tags\": test['tags']}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sc_ds = Dataset.from_dict({\"tokens\": softcite_train['sentences'], \"tags\": softcite_train['tags']}, features=features)\n",
    "validation_sc_ds = Dataset.from_dict({\"tokens\": softc_validation['sentences'], \"tags\": softc_validation['tags']}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = training_ds.features[\"tags\"].feature\n",
    "tag_list = training_ds.features[\"tags\"].feature.names\n",
    "id2tag = {idx: tag for idx, tag in enumerate(all_tags.names)}\n",
    "tag2id = {tag: idx for idx, tag in enumerate(all_tags.names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of models to run\n",
    "m_names = ['microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract']\n",
    "MAX_LENGTH = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://huggingface.co/docs/transformers/tasks/token_classification\n",
    "def tokenize_and_align_labels(data, tknzr, max_length=50):\n",
    "    tokenized_inputs = tknzr(data['tokens'], truncation=True, is_split_into_words=True, max_length=max_length)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(data['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [tag_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [tag_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    f1res = precision_recall_fscore_support(flatten(true_labels), flatten(true_predictions), labels=all_tags.names)\n",
    "\n",
    "    df = list(zip(all_tags.names, f1res[2], f1res[0], f1res[1]))\n",
    "    df = pd.DataFrame(df, columns = ['Level', 'F1-Score', 'Precision', 'Recall'])   \n",
    "    print(df)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4a6c154f624ea1a21e8d82ed38c36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013be29cd1764158a1126611f8c64eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8b3807936d442789135ac5714299c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4095' max='4095' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4095/4095 05:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Tool</th>\n",
       "      <th>Uns Method</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>{'precision': 0.691358024691358, 'recall': 0.8615384615384616, 'f1': 0.7671232876712328, 'number': 65}</td>\n",
       "      <td>{'precision': 0.7543859649122807, 'recall': 0.7413793103448276, 'f1': 0.7478260869565219, 'number': 58}</td>\n",
       "      <td>0.717391</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.998299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.004765</td>\n",
       "      <td>{'precision': 0.7777777777777778, 'recall': 0.8615384615384616, 'f1': 0.8175182481751826, 'number': 65}</td>\n",
       "      <td>{'precision': 0.8035714285714286, 'recall': 0.7758620689655172, 'f1': 0.7894736842105263, 'number': 58}</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.998774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>{'precision': 0.8243243243243243, 'recall': 0.9384615384615385, 'f1': 0.8776978417266187, 'number': 65}</td>\n",
       "      <td>{'precision': 0.7931034482758621, 'recall': 0.7931034482758621, 'f1': 0.7931034482758621, 'number': 58}</td>\n",
       "      <td>0.810606</td>\n",
       "      <td>0.869919</td>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.998833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Level  F1-Score  Precision    Recall\n",
      "0             O  0.999205   0.999424  0.998987\n",
      "1  B-UNS_METHOD  0.796460   0.818182  0.775862\n",
      "2  I-UNS_METHOD  0.813559   0.750000  0.888889\n",
      "3        B-TOOL  0.777778   0.708861  0.861538\n",
      "4        I-TOOL  0.835165   0.826087  0.844444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Level  F1-Score  Precision    Recall\n",
      "0             O  0.999484   0.999444  0.999523\n",
      "1  B-UNS_METHOD  0.814159   0.836364  0.793103\n",
      "2  I-UNS_METHOD  0.821429   0.793103  0.851852\n",
      "3        B-TOOL  0.852941   0.816901  0.892308\n",
      "4        I-TOOL  0.860759   1.000000  0.755556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Level  F1-Score  Precision    Recall\n",
      "0             O  0.999503   0.999642  0.999364\n",
      "1  B-UNS_METHOD  0.807018   0.821429  0.793103\n",
      "2  I-UNS_METHOD  0.809917   0.731343  0.907407\n",
      "3        B-TOOL  0.877698   0.824324  0.938462\n",
      "4        I-TOOL  0.928571   1.000000  0.866667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Level  F1-Score  Precision    Recall\n",
      "0             O  0.998397   0.998939  0.997855\n",
      "1  B-UNS_METHOD  0.729064   0.691589  0.770833\n",
      "2  I-UNS_METHOD  0.705882   0.613636  0.830769\n",
      "3        B-TOOL  0.833333   0.785714  0.887097\n",
      "4        I-TOOL  0.751724   0.721854  0.784173\n"
     ]
    }
   ],
   "source": [
    "test_metrics = dict()\n",
    "\n",
    "for m in m_names:\n",
    "    print(m)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(m)\n",
    "    # dynamically pad sentences to longest length in batch for efficiency\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    train_tokenized = training_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "    val_tokenized = validation_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "    test_tokenized = test_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        m, num_labels=5, id2label=id2tag, label2id=tag2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"model/\" + m,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=val_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    test_preds = trainer.predict(test_tokenized)\n",
    "    test_metrics[m] = test_preds.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract': {'test_loss': 0.018545329570770264,\n",
       "  'test_TOOL': {'precision': 0.7790368271954674,\n",
       "   'recall': 0.8870967741935484,\n",
       "   'f1': 0.8295625942684767,\n",
       "   'number': 310},\n",
       "  'test_UNS_METHOD': {'precision': 0.6548672566371682,\n",
       "   'recall': 0.7708333333333334,\n",
       "   'f1': 0.7081339712918661,\n",
       "   'number': 96},\n",
       "  'test_overall_precision': 0.7489270386266095,\n",
       "  'test_overall_recall': 0.8596059113300493,\n",
       "  'test_overall_f1': 0.8004587155963303,\n",
       "  'test_overall_accuracy': 0.9966448834472571,\n",
       "  'test_runtime': 11.9102,\n",
       "  'test_samples_per_second': 247.938,\n",
       "  'test_steps_per_second': 31.066}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32854cf6dc8b45a5964b5e5c19f1717d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1696 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f594930bda43d091d1608c08e4d623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a8c5b478324199858ead79b10b5d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2953 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='636' max='636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [636/636 01:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Tool</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.021887</td>\n",
       "      <td>{'precision': 0.670995670995671, 'recall': 0.748792270531401, 'f1': 0.7077625570776256, 'number': 207}</td>\n",
       "      <td>0.670996</td>\n",
       "      <td>0.748792</td>\n",
       "      <td>0.707763</td>\n",
       "      <td>0.993733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.025857</td>\n",
       "      <td>{'precision': 0.7549019607843137, 'recall': 0.7439613526570048, 'f1': 0.7493917274939172, 'number': 207}</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.743961</td>\n",
       "      <td>0.749392</td>\n",
       "      <td>0.994142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.036100</td>\n",
       "      <td>0.027507</td>\n",
       "      <td>{'precision': 0.7450980392156863, 'recall': 0.7342995169082126, 'f1': 0.7396593673965937, 'number': 207}</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.734300</td>\n",
       "      <td>0.739659</td>\n",
       "      <td>0.994096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Level  F1-Score  Precision    Recall\n",
      "0             O  0.996963   0.996734  0.997193\n",
      "1  B-UNS_METHOD  0.000000   0.000000  0.000000\n",
      "2  I-UNS_METHOD  0.000000   0.000000  0.000000\n",
      "3        B-TOOL  0.815851   0.788288  0.845411\n",
      "4        I-TOOL  0.551724   0.666667  0.470588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Level  F1-Score  Precision    Recall\n",
      "0             O  0.997081   0.996050  0.998113\n",
      "1  B-UNS_METHOD  0.000000   0.000000  0.000000\n",
      "2  I-UNS_METHOD  0.000000   0.000000  0.000000\n",
      "3        B-TOOL  0.826406   0.836634  0.816425\n",
      "4        I-TOOL  0.538462   0.777778  0.411765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Level  F1-Score  Precision    Recall\n",
      "0             O  0.997080   0.996141  0.998021\n",
      "1  B-UNS_METHOD  0.000000   0.000000  0.000000\n",
      "2  I-UNS_METHOD  0.000000   0.000000  0.000000\n",
      "3        B-TOOL  0.826406   0.836634  0.816425\n",
      "4        I-TOOL  0.537313   0.734694  0.423529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Level  F1-Score  Precision    Recall\n",
      "0             O  0.995815   0.995062  0.996569\n",
      "1  B-UNS_METHOD  0.000000   0.000000  0.000000\n",
      "2  I-UNS_METHOD  0.000000   0.000000  0.000000\n",
      "3        B-TOOL  0.513630   0.462532  0.577419\n",
      "4        I-TOOL  0.314050   0.368932  0.273381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/vy3/conda/envs/flambe/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# softcite run\n",
    "test_sc_metrics = dict()\n",
    "\n",
    "for m in m_names:\n",
    "    print(m)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(m)\n",
    "    # dynamically pad sentences to longest length in batch for efficiency\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    train_sc_tokenized = training_sc_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "    val_sc_tokenized = validation_sc_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "    test_tokenized = test_ds.map(tokenize_and_align_labels, batched=True, fn_kwargs={'tknzr': tokenizer, 'max_length': MAX_LENGTH})\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        m, num_labels=5, id2label=id2tag, label2id=tag2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"model/\" + m,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_sc_tokenized,\n",
    "        eval_dataset=val_sc_tokenized,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    test_preds_sc = trainer.predict(test_tokenized)\n",
    "    test_sc_metrics[m] = test_preds_sc.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract': {'test_loss': 0.036947280168533325,\n",
       "  'test_TOOL': {'precision': 0.4146341463414634,\n",
       "   'recall': 0.5483870967741935,\n",
       "   'f1': 0.4722222222222222,\n",
       "   'number': 310},\n",
       "  'test_UNS_METHOD': {'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'number': 96},\n",
       "  'test_overall_precision': 0.4146341463414634,\n",
       "  'test_overall_recall': 0.4187192118226601,\n",
       "  'test_overall_f1': 0.41666666666666663,\n",
       "  'test_overall_accuracy': 0.9916748040762162,\n",
       "  'test_runtime': 11.7916,\n",
       "  'test_samples_per_second': 250.433,\n",
       "  'test_steps_per_second': 31.378}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sc_metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flambe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
